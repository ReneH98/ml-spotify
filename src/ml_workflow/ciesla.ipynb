{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Genre Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import walk, path\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# get all filenames from the directory data\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"data\"):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "# load data from all files from the directory data\n",
    "frames = []\n",
    "for file in f:\n",
    "    data = pd.read_json(path.join(\"data\", file))\n",
    "    frames.append(data)\n",
    "\n",
    "# concat all data into one dataframe\n",
    "raw_data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\"]\n",
    "categorical_features = [\"key\", \"mode\", \"time_signature\"]\n",
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum and minimum data value within the boxplot whiskers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_factor = 1.5\n",
    "\n",
    "def getQuartiles(data: pd.DataFrame) -> set:\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q1, Q3, IQR\n",
    "\n",
    "def getMaxWhiskerValue(data: pd.Series) -> float:\n",
    "    Q1, Q3, IQR = getQuartiles(data)\n",
    "    whisker_value = Q3 + (IQR * iqr_factor)\n",
    "    return whisker_value\n",
    "\n",
    "def getMinWhiskerValue(data: pd.Series) -> float:\n",
    "    Q1, Q3, IQR = getQuartiles(data)\n",
    "    whisker_value = Q1 - (IQR * iqr_factor)\n",
    "    return whisker_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find correlated features where the correlation coefficient is above a specific threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_features(data:pd.DataFrame, threshold:float) -> list:\n",
    "    correlation_matrix = data.corr().abs()\n",
    "    avg_correlation = correlation_matrix.mean(axis = 1)\n",
    "    up = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    drop = list()\n",
    "        \n",
    "    for row in range(len(up)-1):\n",
    "        for col in range (row + 1, len(up)):\n",
    "            if(correlation_matrix.iloc[row, col] > threshold):\n",
    "                if(avg_correlation.iloc[row] > avg_correlation.iloc[col]): \n",
    "                    drop.append(row)\n",
    "                else: \n",
    "                    drop.append(col)\n",
    "    \n",
    "    drop = list(set(drop)) \n",
    "    dropcols_names = list(data.columns[[item for item in drop]])\n",
    "    \n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove samples with key == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSamplesWithInvalidKey(data: pd.DataFrame) -> None:\n",
    "    old_len = len(data)\n",
    "    data = data[data.key != -1]\n",
    "    print(\"Samples removed because of invalid key:\", old_len - len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove samples where time_signature is out of bounds [3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSamplesWithInvalidTimeSignature(data: pd.DataFrame) -> None:\n",
    "    old_len = len(data)\n",
    "    data = data[(data.time_signature >= 3) & (data.time_signature <= 7)]\n",
    "    print(\"Samples removed because of invalid time_signature:\", old_len - len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary which contains **standard deviation, mean, max and min value within whiskers for every feature**\n",
    "\n",
    "this dictionary is used to preprocess test data in the same way like the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical features (key & time_signature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_features = [\"key\", \"time_signature\"]\n",
    "\n",
    "def encodeCategoricalFeatures(data: pd.DataFrame) -> list:\n",
    "    global preprocessing_categorical_features\n",
    "\n",
    "    enc_features = pd.DataFrame(data[one_hot_encoded_features])\n",
    "\n",
    "    enc_df = pd.DataFrame(preprocessing_one_hot_encoder.fit_transform(enc_features).toarray(), \n",
    "        columns=preprocessing_one_hot_encoder.get_feature_names_out(one_hot_encoded_features), \n",
    "        dtype=int,\n",
    "        index=data.index)\n",
    "    data.drop(one_hot_encoded_features, axis=\"columns\", inplace=True)\n",
    "\n",
    "    data = pd.concat([data, enc_df], axis=\"columns\")\n",
    "\n",
    "    # remove \n",
    "    preprocessing_categorical_features = [e for e in preprocessing_categorical_features if e not in one_hot_encoded_features]\n",
    "    preprocessing_categorical_features = preprocessing_categorical_features + list(enc_df.columns)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def encodeCategoricalFeaturesForTestSet(data: pd.DataFrame):\n",
    "    global preprocessing_one_hot_encoder\n",
    "    \n",
    "    enc_features = pd.DataFrame(data[one_hot_encoded_features])\n",
    "\n",
    "    enc_df = pd.DataFrame(preprocessing_one_hot_encoder.transform(enc_features).toarray(), \n",
    "        columns=preprocessing_one_hot_encoder.get_feature_names_out(one_hot_encoded_features), \n",
    "        dtype=int,\n",
    "        index=data.index)\n",
    "    data.drop(one_hot_encoded_features, axis=\"columns\", inplace=True)\n",
    "    data = pd.concat([data, enc_df], axis=\"columns\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform all preprocessing steps on the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTrainingData(data: pd.DataFrame):\n",
    "    preprocessing_numeric_features = numeric_features\n",
    "    preprocessing_categorical_features = categorical_features\n",
    "    preprocessing_one_hot_encoded_features = [\"key\", \"time_signature\"]\n",
    "\n",
    "    removeSamplesWithInvalidKey(data)\n",
    "    removeSamplesWithInvalidTimeSignature(data)\n",
    "\n",
    "    preprocessing_correlated_features = find_correlated_features(data[preprocessing_numeric_features], .8)\n",
    "    print(f'Drop these correlated features: {preprocessing_correlated_features}')\n",
    "\n",
    "    data.drop(preprocessing_correlated_features, axis=1, inplace=True)\n",
    "    preprocessing_numeric_features = [e for e in preprocessing_numeric_features if e not in preprocessing_correlated_features]\n",
    "\n",
    "    preprocessing_column_transformer = ColumnTransformer([\n",
    "        (\"scaling\", StandardScaler(), preprocessing_numeric_features),\n",
    "        (\"one-hot-encoding\", OneHotEncoder(), preprocessing_one_hot_encoded_features)\n",
    "    ], verbose=True, remainder='passthrough')\n",
    "\n",
    "    transformed = preprocessing_column_transformer.fit_transform(data)\n",
    "\n",
    "    feature_remainder = [e for e in data.columns if e not in preprocessing_numeric_features and e not in preprocessing_one_hot_encoded_features]  \n",
    "\n",
    "    preprocessing_categorical_features = [e for e in preprocessing_categorical_features if e not in preprocessing_one_hot_encoded_features]\n",
    "    one_hot_encoded_features = preprocessing_column_transformer.named_transformers_[\"one-hot-encoding\"].get_feature_names_out(preprocessing_one_hot_encoded_features)\n",
    "    preprocessing_categorical_features.extend(one_hot_encoded_features)\n",
    "\n",
    "    preprocessing_transformed_features = preprocessing_numeric_features.copy()\n",
    "    preprocessing_transformed_features.extend(one_hot_encoded_features)\n",
    "    preprocessing_transformed_features.extend(feature_remainder)\n",
    "\n",
    "    data = pd.DataFrame(transformed, index=data.index, columns=preprocessing_transformed_features)\n",
    "\n",
    "    preprocessing_features_info = {}\n",
    "    preprocessing_features_info[\"max_whisker_value\"] = {}\n",
    "    preprocessing_features_info[\"min_whisker_value\"] = {}\n",
    "\n",
    "    for feature_name in data[preprocessing_numeric_features]:\n",
    "        max_whisker_value = getMaxWhiskerValue(data[feature_name])\n",
    "        min_whisker_value = getMinWhiskerValue(data[feature_name])\n",
    "        preprocessing_features_info[\"max_whisker_value\"][feature_name] = max_whisker_value\n",
    "        preprocessing_features_info[\"min_whisker_value\"][feature_name] = min_whisker_value\n",
    "\n",
    "        # set outliers to min/max whisker\n",
    "        data[feature_name] = data[feature_name].clip(min_whisker_value, max_whisker_value)\n",
    "        \n",
    "    preprocessing_features = preprocessing_numeric_features + preprocessing_categorical_features\n",
    "\n",
    "    print(\"final features\", preprocessing_features)\n",
    "\n",
    "    test_interface = {\n",
    "        \"preprocessing_categorical_features\": preprocessing_categorical_features,\n",
    "        \"preprocessing_numeric_features\": preprocessing_numeric_features,\n",
    "        \"preprocessing_features\" : preprocessing_features,\n",
    "        \"preprocessing_correlated_features\": preprocessing_correlated_features,\n",
    "        \"preprocessing_transformed_features\": preprocessing_transformed_features,\n",
    "        \"preprocessing_features_info\": preprocessing_features_info,\n",
    "        \"preprocessing_column_transformer\": preprocessing_column_transformer\n",
    "    }\n",
    "\n",
    "    return data[preprocessing_features], data.playlist_id, test_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform all preprocessing steps on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTestData(data: pd.DataFrame, test_interface: dict) -> pd.DataFrame:\n",
    "\n",
    "    removeSamplesWithInvalidKey(data)\n",
    "    removeSamplesWithInvalidTimeSignature(data)\n",
    "\n",
    "    data.drop(test_interface[\"preprocessing_correlated_features\"], axis=1, inplace=True)\n",
    "\n",
    "    transformed = test_interface[\"preprocessing_column_transformer\"].transform(data)\n",
    "    data = pd.DataFrame(transformed, columns=test_interface[\"preprocessing_transformed_features\"])\n",
    "\n",
    "    for feature_name in data[test_interface[\"preprocessing_numeric_features\"]]:\n",
    "        max_whisker_value = test_interface[\"preprocessing_features_info\"][\"max_whisker_value\"][feature_name]\n",
    "        min_whisker_value = test_interface[\"preprocessing_features_info\"][\"min_whisker_value\"][feature_name]\n",
    "\n",
    "        # set outliers to min/max whisker\n",
    "        data[feature_name] = data[feature_name].clip(min_whisker_value, max_whisker_value)\n",
    "       \n",
    "    return data[test_interface[\"preprocessing_features\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(raw_data, raw_data.genre, test_size=0.33, stratify=raw_data.genre, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Preprocessing on Training & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed, x_train_playlists, test_interface  = preprocessTrainingData(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocessed = preprocessTestData(x_test, test_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(x_train_preprocessed.astype(float).describe())\n",
    "print(x_test_preprocessed.astype(float).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_preprocessed.max())\n",
    "print(test_interface[\"preprocessing_features_info\"][\"max_whisker_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"x_train\": x_train_preprocessed,\n",
    "    \"x_playlists\": x_train_playlists,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"features\": features,\n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"target\": \"genre\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data[\"x_train\"][data[\"numeric_features\"]] = scaler.fit_transform(data[\"x_train\"][data[\"numeric_features\"]])\n",
    "data[\"x_train\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier detection using zscore\n",
    "threshold = 3\n",
    "filtered_data = pd.DataFrame([])\n",
    "removed_cnt = 0\n",
    "train_combined = data[\"x_train\"].copy()\n",
    "train_combined[\"genre\"] = data[\"y_train\"]\n",
    "\n",
    "for group_name, group_data in train_combined.groupby(\"genre\"):\n",
    "    group_data = group_data.drop(\"genre\", 1)\n",
    "\n",
    "    z_score = group_data.select_dtypes(include='number').apply(stats.zscore)\n",
    "    filter = (abs(z_score) < threshold).all(axis=1)\n",
    "    group_filtered = group_data[filter]\n",
    "    group_filtered['genre'] = group_name\n",
    "\n",
    "    removed_cnt += (group_data.shape[0] - group_filtered.shape[0])\n",
    "    filtered_data = pd.concat([filtered_data, group_filtered], ignore_index=False)\n",
    "\n",
    "print(f\"Removed samples: {removed_cnt}\")\n",
    "\n",
    "data[\"y_train\"] = filtered_data[\"genre\"]\n",
    "data[\"x_train\"] = filtered_data.drop(\"genre\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to compare statistic informations from specific features you can use: data.groupby('genre').describe()[\"feature1\", \"feature2\", ...]\n",
    "data[\"x_train\"].groupby(data[\"y_train\"]).describe()[\"duration_s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"x_train\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = data[\"y_train\"].value_counts().sort_values()\n",
    "labels = sorted_list.index.tolist()\n",
    "values = sorted_list.tolist()\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.title(\"genre\")\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = data[\"x_train\"].copy()\n",
    "train_combined[\"genre\"] = data[\"y_train\"]\n",
    "for feature in data[\"features\"]:\n",
    "    sns.boxplot(x=\"genre\", y=feature, data=train_combined)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"x_train\"].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "heatmap = sns.heatmap(data[\"x_train\"].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12)\n",
    "fig.set_size_inches(15.5, 10.5, forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=train_combined, hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_statistics, chi2_p_scores = chi2(data[\"x_train\"][data[\"categorical_features\"]], data[\"y_train\"])\n",
    "chi2_scores = pd.Series(chi2_statistics, index=data[\"categorical_features\"])\n",
    "chi2_scores.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA f Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_statistics, anova_p_scores = f_classif(data[\"x_train\"][data[\"numeric_features\"]], data[\"y_train\"])\n",
    "anova_scores = pd.Series(anova_statistics, index=data[\"numeric_features\"])\n",
    "anova_scores.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features_indices = [list(data[\"x_train\"].columns).index(x) for x in data[\"categorical_features\"]]\n",
    "mutual_statistics = mutual_info_classif(data[\"x_train\"], data[\"y_train\"], discrete_features=discrete_features_indices)\n",
    "mutual_scores = pd.Series(mutual_statistics, index=data[\"features\"])\n",
    "mutual_scores.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features_indices = [list(x_test.columns).index(x) for x in categorical_features]\n",
    "mutual_statistics = mutual_info_classif(x_train, y_train, discrete_features=discrete_features_indices)\n",
    "mutual_scores = pd.Series(mutual_statistics, index=x_train.columns)\n",
    "mutual_scores.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "forest_importances = pd.Series(clf.feature_importances_, index=x_train.columns)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.sort_values(ascending=False).plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
