{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Genre Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from os import walk, path\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score\n",
    "from scipy import stats\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from cf_matrix import make_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# get all filenames from the directory data\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"data\"):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "\n",
    "# load data from all files from the directory data\n",
    "frames = []\n",
    "for file in f:\n",
    "    data = pd.read_json(path.join(\"data\", file))\n",
    "    frames.append(data)\n",
    "\n",
    "# concat all data into one dataframe\n",
    "raw_data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to compare statistic informations from specific features you can use: data.groupby('genre').describe()[\"feature1\", \"feature2\", ...]\n",
    "raw_data.groupby(raw_data[\"genre\"]).describe()[\"energy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_plots = raw_data.copy()\n",
    "raw_data_plots.drop([\"uri\", \"id\", \"track_href\", \"analysis_url\", \"type\", \"playlist_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_plots.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = raw_data_plots[\"genre\"].value_counts().sort_values()\n",
    "labels = sorted_list.index.tolist()\n",
    "values = sorted_list.tolist()\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.title(f\"No. of samples {raw_data.shape[0]}\")\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in raw_data_plots.iloc[:,:13].columns:\n",
    "    sns.boxplot(x=\"genre\", y=feature, data=raw_data_plots)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_plots.iloc[:,:13].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "heatmap = sns.heatmap(raw_data_plots.iloc[:,:13].corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12)\n",
    "fig.set_size_inches(15.5, 10.5, forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=raw_data_plots, hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\"]\n",
    "categorical_features = [\"key\", \"mode\", \"time_signature\"]\n",
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum and minimum data value within the boxplot whiskers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_factor = 1.5\n",
    "\n",
    "def getQuartiles(data: pd.DataFrame) -> set:\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q1, Q3, IQR\n",
    "\n",
    "def getMaxWhiskerValue(data: pd.Series) -> float:\n",
    "    Q1, Q3, IQR = getQuartiles(data)\n",
    "    whisker_value = Q3 + (IQR * iqr_factor)\n",
    "    return whisker_value\n",
    "\n",
    "def getMinWhiskerValue(data: pd.Series) -> float:\n",
    "    Q1, Q3, IQR = getQuartiles(data)\n",
    "    whisker_value = Q1 - (IQR * iqr_factor)\n",
    "    return whisker_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find correlated features where the correlation coefficient is above a specific threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_features(data:pd.DataFrame, threshold:float) -> list:\n",
    "    correlation_matrix = data.corr().abs()\n",
    "    avg_correlation = correlation_matrix.mean(axis = 1)\n",
    "    up = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    drop = list()\n",
    "        \n",
    "    for row in range(len(up)-1):\n",
    "        for col in range (row + 1, len(up)):\n",
    "            if(correlation_matrix.iloc[row, col] > threshold):\n",
    "                if(avg_correlation.iloc[row] > avg_correlation.iloc[col]): \n",
    "                    drop.append(row)\n",
    "                else: \n",
    "                    drop.append(col)\n",
    "    \n",
    "    drop = list(set(drop)) \n",
    "    dropcols_names = list(data.columns[[item for item in drop]])\n",
    "    \n",
    "    return dropcols_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove samples with key == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSamplesWithInvalidKey(data: pd.DataFrame) -> None:\n",
    "    old_len = len(data)\n",
    "    data = data[data.key != -1]\n",
    "    print(\"Samples removed because of invalid key:\", old_len - len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove samples where time_signature is out of bounds [3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSamplesWithInvalidTimeSignature(data: pd.DataFrame) -> None:\n",
    "    old_len = len(data)\n",
    "    data = data[(data.time_signature >= 3) & (data.time_signature <= 7)]\n",
    "    print(\"Samples removed because of invalid time_signature:\", old_len - len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform all preprocessing steps on the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTrainingData(data: pd.DataFrame):\n",
    "    preprocessing_numeric_features = numeric_features\n",
    "    preprocessing_categorical_features = categorical_features\n",
    "    preprocessing_one_hot_encoded_features = [\"key\", \"time_signature\"]\n",
    "\n",
    "    removeSamplesWithInvalidKey(data)\n",
    "    removeSamplesWithInvalidTimeSignature(data)\n",
    "\n",
    "    # drop features with high correlation coefficient\n",
    "    preprocessing_correlated_features = find_correlated_features(data[preprocessing_numeric_features], .8)\n",
    "    print(f'Drop these correlated features: {preprocessing_correlated_features}')\n",
    "    data.drop(preprocessing_correlated_features, axis=1, inplace=True)\n",
    "\n",
    "    # remove correlated features from numeric features\n",
    "    preprocessing_numeric_features = [e for e in preprocessing_numeric_features if e not in preprocessing_correlated_features]\n",
    "\n",
    "    # create column transformer for scaling and one-hot-encoding\n",
    "    preprocessing_column_transformer = ColumnTransformer([\n",
    "        (\"scaling\", StandardScaler(), preprocessing_numeric_features),\n",
    "        (\"one-hot-encoding\", OneHotEncoder(), preprocessing_one_hot_encoded_features)\n",
    "    ], verbose=True, remainder='passthrough')\n",
    "\n",
    "    # perform scaling and one-hot-encoding\n",
    "    transformed_data = preprocessing_column_transformer.fit_transform(data)\n",
    "\n",
    "    # list containing features which are not used in the column transformer\n",
    "    feature_remainder = [e for e in data.columns if e not in preprocessing_numeric_features and e not in preprocessing_one_hot_encoded_features]  \n",
    "\n",
    "    # update list of categorical features according to one-hot-encoding\n",
    "    preprocessing_categorical_features = [e for e in preprocessing_categorical_features if e not in preprocessing_one_hot_encoded_features]\n",
    "    one_hot_encoded_features = preprocessing_column_transformer.named_transformers_[\"one-hot-encoding\"].get_feature_names_out(preprocessing_one_hot_encoded_features)\n",
    "    preprocessing_categorical_features.extend(one_hot_encoded_features)\n",
    "\n",
    "    # create feature name list containing the new one-hot-encoded features\n",
    "    preprocessing_transformed_features = preprocessing_numeric_features.copy()\n",
    "    preprocessing_transformed_features.extend(one_hot_encoded_features)\n",
    "    preprocessing_transformed_features.extend(feature_remainder)\n",
    "\n",
    "    # create new dataframe with transformed data\n",
    "    data = pd.DataFrame(transformed_data, index=data.index, columns=preprocessing_transformed_features)\n",
    "\n",
    "    # create dictionary which contains min and max whisker values for every feature and clip the data according to them\n",
    "    preprocessing_features_info = {}\n",
    "    preprocessing_features_info[\"max_whisker_value\"] = {}\n",
    "    preprocessing_features_info[\"min_whisker_value\"] = {}\n",
    "\n",
    "    for feature_name in data[preprocessing_numeric_features]:\n",
    "        max_whisker_value = getMaxWhiskerValue(data[feature_name])\n",
    "        min_whisker_value = getMinWhiskerValue(data[feature_name])\n",
    "        preprocessing_features_info[\"max_whisker_value\"][feature_name] = max_whisker_value\n",
    "        preprocessing_features_info[\"min_whisker_value\"][feature_name] = min_whisker_value\n",
    "\n",
    "        # set outliers to min/max whisker\n",
    "        data[feature_name] = data[feature_name].clip(min_whisker_value, max_whisker_value)\n",
    "\n",
    "    # create list containing all features for the training data\n",
    "    preprocessing_features = preprocessing_numeric_features + preprocessing_categorical_features\n",
    "\n",
    "    print(\"final features\", preprocessing_features)\n",
    "\n",
    "    # create a dict containing information which is needed to preprocess future test data\n",
    "    preprocessing_pipeline = {\n",
    "        \"categorical_features\": preprocessing_categorical_features,\n",
    "        \"numeric_features\": preprocessing_numeric_features,\n",
    "        \"features\" : preprocessing_features,\n",
    "        \"correlated_features\": preprocessing_correlated_features,\n",
    "        \"transformed_features\": preprocessing_transformed_features,\n",
    "        \"features_info\": preprocessing_features_info,\n",
    "        \"column_transformer\": preprocessing_column_transformer\n",
    "    }\n",
    "\n",
    "    return data[preprocessing_features], data.playlist_id, preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform all preprocessing steps on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTestData(data: pd.DataFrame, preprocessing_pipeline: dict) -> pd.DataFrame:\n",
    "\n",
    "    removeSamplesWithInvalidKey(data)\n",
    "    removeSamplesWithInvalidTimeSignature(data)\n",
    "\n",
    "    data.drop(preprocessing_pipeline[\"correlated_features\"], axis=1, inplace=True)\n",
    "\n",
    "    transformed = preprocessing_pipeline[\"column_transformer\"].transform(data)\n",
    "    data = pd.DataFrame(transformed, index=data.index, columns=preprocessing_pipeline[\"transformed_features\"])\n",
    "\n",
    "    for feature_name in data[preprocessing_pipeline[\"numeric_features\"]]:\n",
    "        max_whisker_value = preprocessing_pipeline[\"features_info\"][\"max_whisker_value\"][feature_name]\n",
    "        min_whisker_value = preprocessing_pipeline[\"features_info\"][\"min_whisker_value\"][feature_name]\n",
    "\n",
    "        # set outliers to min/max whisker\n",
    "        data[feature_name] = data[feature_name].clip(min_whisker_value, max_whisker_value)\n",
    "       \n",
    "    return data[preprocessing_pipeline[\"features\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(raw_data, raw_data.genre, test_size=0.25, stratify=raw_data.genre, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Preprocessing on Training & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocess training set\")\n",
    "x_train_preprocessed, x_train_playlists, preprocessing_pipeline  = preprocessTrainingData(x_train)\n",
    "\n",
    "print(\"Preprocess test set\")\n",
    "x_test_preprocessed = preprocessTestData(x_test, preprocessing_pipeline)\n",
    "\n",
    "data = {\n",
    "    \"x_train\": x_train_preprocessed,\n",
    "    \"x_playlists\": x_train_playlists,\n",
    "    \"x_test\": x_test_preprocessed,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"features\": preprocessing_pipeline[\"features\"],\n",
    "    \"numeric_features\": preprocessing_pipeline[\"numeric_features\"],\n",
    "    \"categorical_features\": preprocessing_pipeline[\"categorical_features\"],\n",
    "    \"target\": \"genre\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {}\n",
    "\n",
    "estimators['knn'] = {\n",
    "    'estimator': KNeighborsClassifier(),\n",
    "    'paramGrid': {\n",
    "            'n_neighbors' : [x for x in range(3, 40) if x % len(set(data[\"y_train\"])) != 0],\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'metric' : ['euclidean', 'manhattan']\n",
    "            }\n",
    "}\n",
    "\n",
    "estimators['randomForest'] = {\n",
    "    'estimator': RandomForestClassifier(),\n",
    "    'paramGrid': {\n",
    "            'max_depth': [30, 40, 50, 60],\n",
    "            'max_features': [5, 10, 20],\n",
    "            'min_samples_leaf': [1, 2, 3],\n",
    "            'min_samples_split': [3, 5, 8],\n",
    "            'n_estimators': [1000, 2000, 4000]\n",
    "           } \n",
    "}\n",
    "\n",
    "estimators['svc'] = {\n",
    "    'estimator': SVC(probability=True),\n",
    "    'paramGrid': [\n",
    "            {\"kernel\": [\"rbf\"], \"gamma\": np.float_power(10, range(-4,4)), \"C\": np.float_power(3, range(0,6))},\n",
    "            {\"kernel\": [\"linear\"], \"C\":  np.float_power(3, range(0,6))},\n",
    "           ]\n",
    "}\n",
    "\n",
    "results = dict()\n",
    "seed = 12345\n",
    "\n",
    "for estimatorKey, estimatorValue in estimators.items():\n",
    "    results[estimatorKey] = []\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in outer_cv.split(X=data[\"x_train\"], y=data[\"y_train\"]):\n",
    "        X_train, X_test = data[\"x_train\"].iloc[train_index,:], data[\"x_train\"].iloc[test_index,:]\n",
    "        y_train, y_test = data[\"y_train\"].iloc[train_index], data[\"y_train\"].iloc[test_index]\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator = estimatorValue['estimator'], \n",
    "            param_grid = estimatorValue['paramGrid'], \n",
    "            cv = inner_cv, \n",
    "            n_jobs = -1, \n",
    "            verbose = 1\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        res = {}\n",
    "        res[\"acc_score\"] = acc_score\n",
    "        res[\"best_params\"] = json.dumps(grid_search.best_params_)\n",
    "        res[\"best_estimator\"] = grid_search.best_estimator_\n",
    "\n",
    "        results[estimatorKey].append(res)\n",
    "\n",
    "    print(estimatorKey, results[estimatorKey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_variance_threshold = 0.9\n",
    "\n",
    "pca = PCA(pca_variance_threshold)\n",
    "pca.fit(data[\"x_train\"][data[\"numeric_features\"]])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.scatter(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.grid()\n",
    "plt.title(f\"PCA Variance - {np.sum(pca.explained_variance_ratio_)}\")\n",
    "plt.xlabel(\"no of PCA dimensions\") \n",
    "plt.ylabel(\"% variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(pca: PCA, input_data: pd.DataFrame):\n",
    "    pca_components = pca.transform(input_data[data[\"numeric_features\"]])\n",
    "    cols = [f\"PC{x}\" for x in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    pca_df = pd.DataFrame(pca_components, columns=cols, index=input_data.index)\n",
    "    pca_fransformed = pd.concat([pca_df, input_data[data[\"categorical_features\"]]], axis=1)\n",
    "    return pca_fransformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=3, gamma=0.1, kernel=\"rbf\")\n",
    "train_pca = pca_transformation(pca, data[\"x_train\"])\n",
    "svc_model.fit(train_pca, data[\"y_train\"])\n",
    "\n",
    "test_pca = pca_transformation(pca, data[\"x_test\"])\n",
    "predicted = svc_model.predict(test_pca)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(data[\"y_test\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"models/\"\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "for estimator_key, estimators_results in results.items():\n",
    "    for i in range(0, len(estimators_results)):\n",
    "        file_name = dir_name + estimator_key + \"_\" + str(i) + \"_\" + str(estimators_results[i][\"acc_score\"]) + \".joblib\"\n",
    "        dump(estimators_results[i][\"best_estimator\"], file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform CV and train models on entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator_key, estimators_results in results.items():\n",
    "    for estimator_result in estimators_results:\n",
    "        new_model = None\n",
    "\n",
    "        if estimator_key == \"knn\":\n",
    "            new_model = KNeighborsClassifier()\n",
    "        elif estimator_key == \"randomForest\":\n",
    "            new_model = RandomForestClassifier()\n",
    "        elif estimator_key == \"svc\":\n",
    "            new_model = SVC(probability=True, random_state=1)\n",
    "\n",
    "        new_model.set_params(**estimator_result[\"best_estimator\"].get_params())\n",
    "        \n",
    "        cv_scores = cross_val_score(new_model, data[\"x_train\"], data[\"y_train\"], cv=5)\n",
    "\n",
    "        new_model.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "\n",
    "        estimator_result[\"cv_training_acc\"] = cv_scores\n",
    "        estimator_result[\"final_model\"] = new_model\n",
    "\n",
    "        print(estimator_key, estimator_result[\"best_params\"], cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_results = []\n",
    "for i in range(1, len(data[\"features\"])):\n",
    "\n",
    "    svc = SVC(C=3, gamma=0.1, kernel=\"rbf\")\n",
    "    sfs_forward = SequentialFeatureSelector(svc, n_features_to_select=i, direction=\"forward\").fit(data[\"x_train\"], data[\"y_train\"])\n",
    "\n",
    "    feature_names = list(sfs_forward.get_feature_names_out())\n",
    "\n",
    "    cv_score = cross_val_score(svc, data[\"x_train\"][feature_names], data[\"y_train\"], cv=5)\n",
    "\n",
    "    feature_selection_results.append(\n",
    "        {\n",
    "            \"feature_cnt\": i,\n",
    "            \"cv_score\": cv_score\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f'No. of features: {i}, Features: {feature_names}, Score: {cv_score.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selection_results\n",
    "\n",
    "cnts = [x[\"feature_cnt\"] for x in feature_selection_results]\n",
    "scores = [x[\"cv_score\"].mean() for x in feature_selection_results]\n",
    "\n",
    "ax = sns.lineplot(x=cnts, y=scores)\n",
    "ax.set_title(\"Forward Feature Selection\")\n",
    "ax.set_xlabel(\"Number of features\")\n",
    "ax.set_ylabel(\"CV Accuracy\")\n",
    "plt.savefig(\"feature_selection.png\")\n",
    "\n",
    "for i in range(0,25):\n",
    "    print(cnts[i])\n",
    "    print(scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots over CV accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for estimator_key, estimators_results in results.items():\n",
    "    result_cv_scores = {}\n",
    "    params = {}    \n",
    "\n",
    "    for estimator_result in estimators_results:\n",
    "        if estimator_result[\"best_params\"] not in params.values():\n",
    "            params[i] = estimator_result[\"best_params\"]\n",
    "            result_cv_scores[i] = estimator_result[\"cv_training_acc\"] \n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    result_cv_scores = pd.DataFrame.from_dict(result_cv_scores)\n",
    "\n",
    "    ax = sns.boxplot(data=result_cv_scores, palette=\"magma\")   \n",
    "    ax.set_title(f\"{estimator_key} CV accuracies\")\n",
    "\n",
    "    for param_num, param_val in params.items():\n",
    "        print(f\"{param_num}: {param_val}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices and dataframe containing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval = [] \n",
    "\n",
    "for estimator_key, estimators_results in results.items():\n",
    "    for estimator_result in estimators_results:\n",
    "        y_pred = estimator_result[\"final_model\"].predict(data[\"x_test\"])\n",
    "        y_pred_proba = estimator_result[\"final_model\"].predict_proba(data[\"x_test\"])\n",
    "\n",
    "        result_eval.append(\n",
    "            {\n",
    "                \"model\": estimator_key,\n",
    "                \"params\": estimator_result[\"best_params\"],\n",
    "                \"nested_cv_training_acc\": estimator_result[\"acc_score\"],\n",
    "                \"cv_training_acc\": estimator_result[\"cv_training_acc\"].mean(),\n",
    "                \"test_acc\": accuracy_score(data[\"y_test\"], y_pred),\n",
    "                \"test_roc_auc\": roc_auc_score(data[\"y_test\"], y_pred_proba, multi_class=\"ovr\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        cf_matrix_title = f\"{estimator_key} {estimator_result['best_params']}\"\n",
    "        cf_matrix = confusion_matrix(data[\"y_test\"], y_pred)\n",
    "        make_confusion_matrix(cf_matrix, figsize=(8,6), cbar=False, title=cf_matrix_title, categories=estimator_result[\"final_model\"].classes_)\n",
    "         \n",
    "result_eval = pd.DataFrame(result_eval)\n",
    "result_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = results[\"randomForest\"][0][\"best_estimator\"]\n",
    "y_pred = final_model.predict(data[\"x_test\"])\n",
    "\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
